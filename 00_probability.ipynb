{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:65279b9a6bb73a617beb86a412887b3140ab2c6be0eae769799c20debe63a0f0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Probability Theory for Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<center>\n",
      "**There is (almost) no machine learning algorithm that cannot be interpreted from a probabilistic perspective.**\n",
      "</center>\n",
      "\n",
      "For some algorithms that will be more obvious than for others, e.g.\n",
      "\n",
      "* Gaussian Mixture Models (Clustering, Regression)\n",
      "* Naive Bayes (Classification)\n",
      "* Linear Regression (Regression)\n",
      "* Logistic Regression (Classification)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Probability\n",
      "----------\n",
      "\n",
      "Let $A$ be the outcome of a random experiment. $P(A)$ is its probability and\n",
      "\n",
      "* $0 \\leq P(A) \\leq 1$\n",
      "* $P(A) = 1$ means the random experiment will **always** have the outcome $A$\n",
      "* $P(A) = 0$ means the random experiment will **never** have the outcome $A$\n",
      "* $P(A) = p \\in (0, 1)$ means the random experiment will have the outcome $A$ with probability $p$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Interpretations\n",
      "--------------\n",
      "\n",
      "**Bayesian**: $P(A) = 0.9$ means we **believe** that the outcome of the random experiment is $A$ with 90 % confidence.\n",
      "\n",
      "**Frequentist**: $P(A) = 0.9$ means if we repeated the random experiment **infinitely** often we would get the outcome $A$ in 90 % of all cases."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Joint Distribution\n",
      "----------------\n",
      "\n",
      "Probability of observing $A$ **and** $B$\n",
      "\n",
      "$$P(A, B) = P(A) P(B)$$\n",
      "\n",
      "Conditional Distribution\n",
      "----------------------\n",
      "\n",
      "Probability of observing $A$ given $B$ has been observed\n",
      "\n",
      "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
      "\n",
      "Independence\n",
      "-----------\n",
      "\n",
      "Outcomes $A$ and $B$ (with probability greater than 0) are **independend** ($A \\perp B$) iff (if and only if)\n",
      "\n",
      "$$P(A|B) = P(A) \\text{ which is the same as } P(A \\cap B) = P(A) P(B)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bayes' Theorem\n",
      "-------------\n",
      "\n",
      "$$P(B|A) = \\frac{P(A|B) P(B)}{P(A)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Some Vocabularies\n",
      "---------------\n",
      "\n",
      "Let\n",
      "\n",
      "* $\\mathcal{D}$ be a set of observations (e.g. a training set),\n",
      "* $H$ is a **hypothesis**\n",
      "\n",
      "We call\n",
      "\n",
      "* $P(H)$ is a **prior**; how likely is the hypothesis $H$ without any observations?\n",
      "* $P(\\mathcal{D})$ the **evidence**; usually all observations are equiprobable\n",
      "* $P(\\mathcal{D} | H)$\n",
      "    * probability of the observations for fixed $H$\n",
      "    * **likelihood** of the hypothesis for fixed $\\mathcal{D}$, also $P(\\mathcal{D} | H) = \\mathcal{L}(H | \\mathcal{D})$\n",
      "* $P(H | \\mathcal{D})$ the posterior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Bayesian Inference\n",
      "----------------\n",
      "\n",
      "$$P(H|\\mathcal{D}) = \\frac{P(\\mathcal{D}|H) P(H)}{P(\\mathcal{D})} = \\frac{\\mathcal{L}(H | \\mathcal{D}) P(H)}{P(\\mathcal{D})}$$\n",
      "\n",
      "$$\\text{posterior} = \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{evidence}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What Does Learning Mean?\n",
      "----------------------\n",
      "\n",
      "Learning means optimizing the hypothesis with respect to a given objective, e.g.\n",
      "\n",
      "Maximum a Postiori Estimate (MAP)\n",
      "------------------------------\n",
      "\n",
      "$H_{\\text{MAP}} = \\arg \\max_H \\mathcal{L}(H | \\mathcal{D}) P(H)$\n",
      "\n",
      "Maximum Likelihood Estimate (MLE)\n",
      "------------------------------\n",
      "\n",
      "$H_{\\text{MLE}} = \\arg \\max_H \\mathcal{L}(H | \\mathcal{D})$\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "* the evidence $P(\\mathcal{D})$ does not change the maximum\n",
      "* often the log-likelihood $\\log \\mathcal{L}$ will be optimized\n",
      "* MAP is often a **regularized** version of the MLE, i.e. it generalizes better over unseen data\n",
      "* MLE is a special case of MAP with uniform prior"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Moments of random variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Mode** - the value that appears most often in a dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Mean** - the average value of a random variable X\n",
      "\n",
      "$$\\mu_x = E(X) = \\sum_x p(x) \\cdot x$$\n",
      "\n",
      "Lemma: $E(a + b X) = a + b E(X)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Variance** - squared average deviation from the mean of a random variable\n",
      "\n",
      "$$Var(X) = E([X - E(X)]^2) = E(X^2) - \\mu_x^2$$\n",
      "\n",
      "Lemma: $Var(a + b X) = b^2 Var(X)$\n",
      "\n",
      "**Standard deviation** - square root of the variance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Estimated Moments from Samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.random.seed(0)\n",
      "x = np.random.randn(1000) * np.random.randn() + np.random.randn()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimated_mean = np.sum(x) / 1000.0\n",
      "estimated_mean"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.86731284696043887"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(x)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.86731284696043887"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimated_var = np.sum((x - estimated_mean) ** 2) / 1000.0\n",
      "estimated_var"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.30113051335498248"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.var(x)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.30113051335498248"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "References\n",
      "---------\n",
      "\n",
      "[1] Murphy, Kevin P.: Machine Learning - A Probabilistic Perspective, 2012, MIT Press."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}