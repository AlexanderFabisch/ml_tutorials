{
 "metadata": {
  "name": "01_linear_regression"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup\n",
      "=====\n",
      "\n",
      "Required packages to run this notebook:\n",
      "\n",
      "* Python\n",
      "* IPython - interactive Python shell\n",
      "* IPython notebooks - IPython for the browser\n",
      "* NumPy - linear algebra library\n",
      "* Matplotlib - plotting library\n",
      "\n",
      "For Windows: use the [Enthought Python Distribution](https://www.enthought.com/products/epd/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Your Implementations\n",
      "====================\n",
      "\n",
      "... should be written in any programming language that are **freely** available (i.e. **NOT** Matlab), e.g.\n",
      "\n",
      "* Python\n",
      "* [Julia](http://julialang.org/)\n",
      "* Octave (free Matlab clone)\n",
      "* C/C++\n",
      "* Java\n",
      "* ...\n",
      "\n",
      "Remember to provide detailed instructions on how to compile and run your programs. Otherwise I will have to contact you and this will slow down the correction of your solutions.\n",
      "\n",
      "It helps **a lot** to use libraries that have support for linear algebra (matrices and vectors), e.g.\n",
      "\n",
      "* Python: NumPy\n",
      "* Octave: builtin\n",
      "* Julia: builtin\n",
      "* C++: [Eigen **3**](http://eigen.tuxfamily.org/index.php?title=Main_Page)\n",
      "* C/Fortran: BLAS ([OpenBLAS](http://www.openblas.net/), CBLAS, ...), [LAPack](http://www.netlib.org/lapack/)\n",
      "* Java: [JBlas](http://mikiobraun.github.io/jblas/) (BLAS wrapper), [Jama](http://math.nist.gov/javanumerics/jama/)\n",
      "* Ruby: [NMatrix](https://github.com/SciRuby)\n",
      "* ..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "k-Fold Cross-Validation\n",
      "======================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('<iframe src=http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29?useformat=mobile width=700 height=350></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Enable inline plots\n",
      "%pylab inline\n",
      "# Fix seed for random number generator to make the results repeatable\n",
      "random.seed(0)\n",
      "\n",
      "# Utility functions\n",
      "def build_sinusoidal(n_degree, w):\n",
      "    from itertools import chain\n",
      "    # That does not look readable but it works :)\n",
      "    return (((\"%+.2f \\sin(%d /\" + str(n_degree) + \" \\pi x)\") * (n_degree+1)) % tuple(chain(*zip(w, range(n_degree+1)))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear Regression\n",
      "=================\n",
      "\n",
      "* We have samples $\\left(x^{(n)}, y^{(n)}\\right), n = 1, \\ldots, N$ drawn from an unknown function $f(x) = y$, where $x \\in \\mathbb{R}^D, y \\in \\mathbb{R}$. This is called training set $T$.\n",
      "* The goal is to learn a mapping $x^{(n)} \\rightarrow y^{(n)}\\quad \\forall \\left(x^{(n)}, y^{(n)}\\right) \\in T$. $x^{(n)}$ is the input and $y^{(n)}$ is the corresponding desired output (target).\n",
      "\n",
      "Example\n",
      "-------\n",
      "\n",
      "Dataset: line + noise"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Size of the data set\n",
      "N = 5\n",
      "# Inputs, shape: N x 1\n",
      "X = linspace(0.0, 1.0, N).reshape((N, 1))\n",
      "# Outputs, shape: N\n",
      "y = linspace(7.0, -5.0, N) + 2*random.randn(N)\n",
      "\n",
      "print(\"X =\")\n",
      "print(X)\n",
      "print(\"y =\")\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(X[:, 0], y, \"o\")\n",
      "xlabel(\"x\")\n",
      "ylabel(\"y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear Model\n",
      "------------\n",
      "\n",
      "* We can write our constraints down with a **linear** model: $$\\forall n \\in \\{1,\\ldots,N\\}:\\quad x^{(n)T} \\boldsymbol{w} = y^{(n)},$$ where $\\boldsymbol{w}$ is a weight vector.\n",
      "* Weights are parameters of the linear model that can be tuned to fit the training data better.\n",
      "* A shorter version of this expression is the following equation:\n",
      "$$\\boldsymbol{X} \\cdot \\boldsymbol{w} = \\boldsymbol{y}, \\qquad \\boldsymbol{X} \\in \\mathbb{R}^{N \\times D}, \\boldsymbol{y} \\in \\mathbb{R}^N, \\boldsymbol{w} \\in \\mathbb{R}^D,$$\n",
      "where the $n$-th **row** of $\\boldsymbol{X}$ represents $x^{(n)}$ and the $n$-th entry of the vector $\\boldsymbol{y}$ represents $y^{(n)}$.\n",
      "* It might not be possible to find a weight vector that satisfies the constraints perfectly. Instead, we can minimize the sum of squared errors (SSE): $$\\hat{\\boldsymbol{w}} = \\text{argmin}_\\boldsymbol{w} \\dfrac{1}{2}||\\boldsymbol{X}\\boldsymbol{w} - \\boldsymbol{y}||^2_2,$$ where $||\\boldsymbol{A}||_2$ is called [Frobenius norm](http://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) and is the generalization of the Euclidean norm for matrices.\n",
      "\n",
      "Learning Weights\n",
      "----------------\n",
      "\n",
      "There are two ways to adjust the weights of a linear model (which includes linear models with nonlinear features):\n",
      "\n",
      "* **Normal equations** (analytical solution), requires inversion of $D \\times D$ matrix, i.e. does not scale well with the number of features\n",
      "* **Gradient descent** (iterative solution), requires the calculation of $N$ gradients in each step, i.e. does not scale well with the number of examples\n",
      "\n",
      "In addition, we can add a constraint to the objective function to penalize large weights:\n",
      "\n",
      "* **Tikhonov regularization**\n",
      "\n",
      "Normal Equations\n",
      "----------------\n",
      "\n",
      "Solving the equation $\\boldsymbol{X} \\cdot \\boldsymbol{w} = \\boldsymbol{y}$ directly for $\\boldsymbol{w}$ by inversion is not possible, because $\\boldsymbol{X}$ is not necessarily a square matrix, i.e. $\\boldsymbol{w} = \\boldsymbol{X}^{-1} \\boldsymbol{y}$ is usually not possible. In addition, it is usually not possible to find an exact solution. Instead, we find the [least squares](http://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29) solution, which is the solution of\n",
      "\n",
      "$$\\boldsymbol{X}^T\\boldsymbol{X} \\hat{\\boldsymbol{w}} = \\boldsymbol{X}^T\\boldsymbol{y}$$\n",
      "\n",
      "for $\\hat{\\boldsymbol{w}}$, i.e.\n",
      "\n",
      "$$\\hat{\\boldsymbol{w}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}$$\n",
      "\n",
      "*Hint:* The expression $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T$ is equivalent to the [Moore-Penrose pseudoinverse](http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse) $\\boldsymbol{X}^+$ of $\\boldsymbol{X}$. It is a generalization of the inverse for non-square matrices. You can use this to implement normal equations if your library provides the function. You could also use a solver for linear equations (e.g. numpy.linalg.solve or dgesv/zgesv from LAPACK)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normal_equations(X, y):\n",
      "    w = linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "    # ... or we can use the Moore-Penrose pseudoinverse\n",
      "    #w = linalg.pinv(X).dot(y)\n",
      "    # ... or we use the solver\n",
      "    #w = linalg.solve(X.T.dot(X), X.T.dot(y))\n",
      "    return w\n",
      "\n",
      "# Add bias to each row/instance:\n",
      "# x11 x12 ...          x11 x12 ... 1\n",
      "# x21 x22 ...   --->   x21 x22 ... 1\n",
      "# .                    .\n",
      "# .                    .\n",
      "# .                    .\n",
      "X_bias = hstack((X, ones((N, 1))))\n",
      "w = normal_equations(X_bias, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(X_bias[:, 0], y, \"o\")\n",
      "plot(X_bias[:, 0], X_bias.dot(w), \"-\", linewidth=3)\n",
      "xlabel(\"x\")\n",
      "ylabel(\"y\")\n",
      "title(\"Model: $y = %.2f x + %.2f$\" % tuple(w))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Normal Equations - Step by Step\n",
      "-------------------------------\n",
      "\n",
      "In the exercises you have to write down\n",
      "\n",
      "* $\\boldsymbol{X}$ (extended with bias) and $\\boldsymbol{y}$\n",
      "* $\\boldsymbol{X}^T \\boldsymbol{X}$\n",
      "* $(\\boldsymbol{X}^T \\boldsymbol{X})^{-1}$\n",
      "* $(\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T$\n",
      "* $\\hat{\\boldsymbol{w}} = (\\boldsymbol{X}^T \\boldsymbol{y})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$\n",
      "\n",
      "*Hint:* The [inverse of a 2x2 matrix](http://en.wikipedia.org/wiki/Invertible_matrix#Inversion_of_2.C3.972_matrices) is given by\n",
      "\n",
      "    A = [a b]\n",
      "        [c d]\n",
      "\n",
      "    Ainv = 1 / (ad - bc) * [ d -b]\n",
      "                           [-c  a]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"X =\"\n",
      "print X_bias\n",
      "print \"y =\"\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"X^T X =\"\n",
      "print X_bias.T.dot(X_bias)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"(X^T X)^{-1} =\"\n",
      "print linalg.inv(X_bias.T.dot(X_bias))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"(X^T X)^{-1} X^T =\"\n",
      "print linalg.inv(X_bias.T.dot(X_bias)).dot(X_bias.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"w = (X^T X)^{-1} X^T y =\"\n",
      "print linalg.inv(X_bias.T.dot(X_bias)).dot(X_bias.T).dot(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradient Descent\n",
      "----------------\n",
      "\n",
      "Update $\\boldsymbol{w}_t$ incrementally:\n",
      "\n",
      "$$\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t - \\alpha \\nabla \\boldsymbol{w}_t,$$\n",
      "\n",
      "where\n",
      "\n",
      "* $\\alpha$ is a hyperparameter called \"learning rate\", it has to be set manually and must usually be within $]0, 1[$, typical values are 10e-1, 10e-2, 10e-3, ...\n",
      "* $\\nabla \\boldsymbol{w}_t$ is the **gradient** of $\\boldsymbol{w}_t$, i.e. the $i$-th entry of $\\Delta \\boldsymbol{w}_t$ is the derivative of the error function $E(\\boldsymbol{X}, \\boldsymbol{y};\\boldsymbol{w}_t)$ with respect to the weight $w_i$: $$\\Delta {\\boldsymbol{w}_t}_i = \\frac{\\partial E(\\boldsymbol{X}, \\boldsymbol{y};\\boldsymbol{w}_t)}{\\partial {w_t}_i}$$\n",
      "\n",
      "Gradient descent actually is a more general rule that can be applied to any minimization problem if the gradient can be computed (e.g. for artificial neural networks, support vector machines, k-means, ...).\n",
      "\n",
      "Gradient of the Linear Model\n",
      "----------------------------\n",
      "\n",
      "Error function: sum of squared errors (SSE)\n",
      "$$E(\\boldsymbol{X}, \\boldsymbol{y};\\boldsymbol{w}) = \\frac{1}{2}||\\boldsymbol{X} \\cdot \\boldsymbol{w} - \\boldsymbol{y}||^2_2$$\n",
      "\n",
      "Gradient\n",
      "$$\\nabla \\boldsymbol{w} = \\nabla_{\\boldsymbol{w}} E(\\boldsymbol{X}, \\boldsymbol{y};\\boldsymbol{w}) = \\boldsymbol{X}^T \\cdot (\\boldsymbol{X} \\cdot \\boldsymbol{w} - \\boldsymbol{y})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient(X, y, w):\n",
      "    return X.T.dot(X.dot(w) - y)\n",
      "\n",
      "def gradient_descent(X, y, w0, alpha, n_iterations):\n",
      "    w = w0.copy()\n",
      "    history = []\n",
      "    for _ in xrange(n_iterations):\n",
      "        w -= alpha * gradient(X, y, w) # Gradient descent update\n",
      "        history.append(w.copy())\n",
      "    return w, history\n",
      "\n",
      "w, history = gradient_descent(X_bias, y, w0=numpy.zeros(2), alpha=0.1, n_iterations=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(X_bias[:, 0], y, \"o\")\n",
      "plot(X_bias[:, 0], X_bias.dot(w), \"-\", linewidth=3)\n",
      "for i in xrange(0, len(history), 20):\n",
      "    plot(X_bias[:, 0], X_bias.dot(history[i]), \"-\", linewidth=3)\n",
      "xlabel(\"x\")\n",
      "ylabel(\"y\")\n",
      "title(\"Model: $y = %.2f x + %.2f$\" % tuple(w))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tikhonov Regularization\n",
      "-----------------------\n",
      "\n",
      "* Linear least squares: $$\\hat{\\boldsymbol{w}} = \\text{argmin}_\\boldsymbol{w} \\dfrac{1}{2}||\\boldsymbol{X}\\boldsymbol{w} - \\boldsymbol{y}||^2_2$$\n",
      "* Linear least squares + Tikhonov Regularization: $$\\hat{\\boldsymbol{w}} = \\text{argmin}_\\boldsymbol{w} \\dfrac{1}{2}||\\boldsymbol{X}\\boldsymbol{w} - \\boldsymbol{y}||^2_2 + ||\\Gamma w||^2_2$$\n",
      "* In many cases, $\\boldsymbol{\\Gamma}$ is chosen to be diagonal, e.g. $\\boldsymbol{\\Gamma} = diag(\\gamma)$ for some $\\gamma \\in \\mathbb{R}$\n",
      "* For $\\gamma = 0$: Equal to (unregularized) linear least squares\n",
      "* For $\\gamma > 0$: Giving preference to solutions $w$ with smaller norms (stronger for larger $\\gamma$)\n",
      "* Normal equation solution: $$\\hat{\\boldsymbol{w}} = (\\boldsymbol{X}^T \\boldsymbol{X} + \\boldsymbol{\\Gamma}^T \\boldsymbol{\\Gamma})^{-1}\\boldsymbol{X}^T \\boldsymbol{y}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tikhonov(X, y, gamma):\n",
      "    Gamma = numpy.eye(X.shape[1]) * gamma\n",
      "    w = linalg.inv(X.T.dot(X) + Gamma.T.dot(Gamma)).dot(X.T).dot(y)\n",
      "    return w\n",
      "\n",
      "results = [(tikhonov(X_bias, y, gamma=gamma), gamma) for gamma in [0.0, 0.1, 0.5, 1.0, 5.0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(X_bias[:, 0], y, \"o\")\n",
      "for (w_reg, gamma) in results:\n",
      "    plot(X_bias[:, 0], X_bias.dot(w_reg), \"-\", linewidth=3, label=\"$\\gamma = %.2f$\" % gamma)\n",
      "xlabel(\"x\")\n",
      "ylabel(\"y\")\n",
      "legend(loc=\"best\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Approximation of Nonlinear Functions (with a Linear Model)\n",
      "----------------------------------------------------\n",
      "\n",
      "To approximate nonlinear funtions with a linear model, we have to generate **nonlinear features**. In this example, we generate **sinusoidal** features. You could also try radial basis functions, polynomials, ...\n",
      "\n",
      "We expand each feature $x$ to the nonlinear feature vector $(\\cos(\\frac{0}{d} \\pi x), \\cos(\\frac{1}{d} \\pi x), \\ldots, \\cos(\\frac{d}{d} \\pi x))^T$, where $d$ is the degree of the polynomial. Note that $\\cos \\left(\\frac{0}{D} \\pi x \\right)=1$ is the bias we added manually in the previous example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sinusoidalize(X, n_degree):\n",
      "    X_sinusoidal = numpy.ndarray((N, n_degree+1))\n",
      "    for d in range(n_degree+1):\n",
      "        X_sinusoidal[:, d] = numpy.cos(X[:, 0] * numpy.pi * d / n_degree)\n",
      "    return X_sinusoidal\n",
      "\n",
      "n_degree = 5\n",
      "X_sinusoidal = sinusoidalize(X, n_degree=n_degree)\n",
      "w_sin = normal_equations(X_sinusoidal, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(X_bias[:, 0], y, \"o\")\n",
      "#plot(X_bias[:, 0], X_bias.dot(w), \"-\", linewidth=3) # Uncomment to compare with linear approximation\n",
      "plot(X_bias[:, 0], X_sinusoidal.dot(w_sin), \"-\", linewidth=3, color=\"r\")\n",
      "xlabel(\"x\")\n",
      "ylabel(\"y\")\n",
      "title(\"Model: $y = \" + build_sinusoidal(n_degree, w_sin) + \"$\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_sin_regularized = tikhonov(X_sinusoidal, y, gamma=1.0)\n",
      "plot(X_bias[:, 0], y, \"o\")\n",
      "#plot(X_bias[:, 0], X_bias.dot(w), \"-\", linewidth=3) # Uncomment to compare with linear approximation\n",
      "plot(X_bias[:, 0], X_sinusoidal.dot(w_sin), \"-\", linewidth=3, color=\"r\")\n",
      "plot(X_bias[:, 0], X_sinusoidal.dot(w_sin_regularized), \"-\", linewidth=3, color=\"y\")\n",
      "xlabel(\"x\")\n",
      "ylabel(\"y\")\n",
      "title(\"Model: $y = \" + build_sinusoidal(n_degree, w_sin_regularized) + \"$\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}